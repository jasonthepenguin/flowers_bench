
# FlowersBench


FlowersBench is a public, portfolio project for benchmarking AI models, featuring a leaderboard, model comparison tools, and an admin panel. Built with Next.js, Supabase, and Tailwind CSS, it provides a fun and ethical way to compare LLMs, originally created for a friend to track and evaluate models.

[ðŸŒ¸ Live Site](https://flowersbench.com)

<img width="1453" height="1189" alt="Pasted Graphic 5" src="https://github.com/user-attachments/assets/b701df66-b157-46b4-8723-33ca645bb17b" />

## Features

- ðŸŒ¸ **Leaderboard:** View and rank AI models by score, organization, and more.
- ðŸ“ **Admin Panel:** Secure login for admins to add, edit, or remove leaderboard entries and feature tweets.
- ðŸ§‘â€ðŸ’» **Model Comparison:** Greentext and UI benchmarking pages to compare model outputs side-by-side.
- ðŸ¦ **Featured Tweet:** Display a highlighted tweet on the homepage, editable by admins.
- ðŸ”’ **Authentication:** Only admins can access editing features.
- ðŸ“± **Responsive Design:** Works great on desktop and mobile.
- ðŸš€ **Deployed on Vercel**

## Tech Stack

- [Next.js (App Router)](https://nextjs.org/)
- [React](https://react.dev/)
- [Supabase](https://supabase.com/) (Auth & Database)
- [Tailwind CSS](https://tailwindcss.com/)
- [OpenRouter/AI SDK](https://openrouter.ai/)
- [Vercel](https://vercel.com/) (Deployment)


## License

This project is public for portfolio/demo purposes, but not open for contributions. No license is provided.  
This project is not licensed for reuse or modification.

## Credits

- Built by Jason Botterill for @flowersslop
- Inspired by the need for transparent, fun LLM benchmarking

---

> â€œThe most ethical benchmarking of AI models under evaluation.â€  
> â€” FlowersBench
